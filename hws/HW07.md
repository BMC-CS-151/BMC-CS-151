---
layout: default
title: Hashtables, Sorting and Complexity
type: Homework
number: 07
active_tab: homework
release_date: 2023-12-01
due_date: 2023-12-08 23:59:00EDT

---

<!-- Check whether the assignment is ready to release -->
{% capture today %}{{'now' | date: '%s'}}{% endcapture %}
{% capture release_date %}{{page.release_date | date: '%s'}}{% endcapture %}
{% if release_date > today %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->


<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->



{% if page.materials %}
<div class="alert alert-info">
You can download the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>


<i>Remember to make a copy of the notebook into your own Drive by choosing “Save a Copy in Drive” from Colab’s “File” menu.</i>

</div>
{% endif %}





{{page.type}} {{page.number}}: {{page.title}}
=============================================================

_Due: {{page.due_date}}_

## Overview
In this assignment, we’ll explore hashtables, sorting and compare the efficiency
of different methods of removing data duplication. Note that the data set we will
be working with this time is very large (685,725 rows and 116 columns). Using
Excel or Numbers to look at it is NOT recommended. It will take forever to open.
Vim is really your best option here. For the same reason, you
should write efficient code - pay attention to the big-O and no unnecessary loops!

## 1. Data Deduplication

One important, and potentially expensive task when handling data is data 
deduplication, whose goal is to make sure that each item in a data set only 
appears once. We’ll explore three different ways this can be done and determine which one
is most efficient.

### 1.1 The Data: Stop, Question, and Frisk

The New York City police department follows a practice of regularly stopping,
questioning, and frisking people on the street. A 2011 lawsuit against this practice
by the NYCLU successfully curbed the police department’s frequent use of this
practice, though concerns remain. 

We’ll be looking at the data collected in 2011 at the height of the NYPD’s
practice of stop-and-frisk. Each row in the data represents a single stop by a
police officer, and the attributes (columns) include information about the stop
(e.g., whether a weapon was found), information about the person stopped (e.g.,
age, race, etc.), and information about the stop location (e.g., the address). The
dataset can be found via
the NYPD website:

https://www.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip

along with the codebook describing what is included in the data:

https://www1.nyc.gov/assets/nypd/downloads/zip/analysis and planning/stop-
question-frisk/SQF-File-Documentation.zip

### 1.2 Determining Equality

The deduplication goal we will consider here is to collect a list of individuals who
were stopped during the year. One way to think about this is that we would like
to know which people were stopped multiple times by the NYPD in 2011.

In order to determine if two rows contain the same person, we need to develop
a method that checks the information that should be unique to each person and
compares it to determine equality. How exactly the quality test works is entirely
upto you. You should decide what information (which columns) to use by 
examining the row data (while referencing the codebook) carefully, or deduplication will fail.

**Requirements:**

1. Design a class to hold a single row from the CSV. It does not need to hold
all fields in the CSV - you should store only the fields that will be necessary
in order to deduplicate the data.
2. Implement the `Comparable` interface by providing a `compareTo` function in
the class you just created that returns 0 if and only if the compared items are
the same person according to the way you have decided to check uniqueness.
3. Describe and justify how you are determining uniqueness in your README file.

### 1.3 Data Storage

**Requirements:**

1. Make a class to store the full data set. Your constructor for this class should
take the name of a CSV file as input and should do the work of reading in the
data from the CSV and parsing it into an `ArrayList` containing the objects
you developed in the previous section.

### 1.4 Deduplication Methods
You will add five data deduplication methods to the data set class you created
in the previous section. Each of these methods should return an ArrayList of
your designed objects that contains only the non-duplicated individuals who were
stop-and-frisked.

#### 1.4.1 All Pairs Deduplication
One way to find the duplicates in a data set is to
compare each item to each other item in the data set, checking duplicates as you
go. Make sure not to count the comparison of an item to itself as a duplicate. We
will call this the “all pairs” method of data deduplication.

**Requirements**

1. Create a method that uses this “all pairs” method of deduplication to return
a list of non-duplicated items in a given data set. The method signature
should be: `ArrayList<E> allPairsDeduplication()` where `E` can be substituted with the specific object you have created to store
a single row.

#### 1.4.2 Hash Table Deduplication 
Another way to find the duplicates is to create a
key for each item in the data set and insert these items into a hashtable. Items
hashing to the same key will overwrite each other, therefore resulting in 
deduplication. The choice of key will determine whether two items are considered 
to be
duplicates, so choose it carefully.


***Requirements:***

1. Create a method that uses this hashtable-based method of deduplication
to return a list of non-duplicate items in a given data set. You may use
the `ProbeHashMap.java` from the book. You program should first creates
a `ProbeHashMap` with the size 1000003 (use the one-parameter constructor)
and then insert the items into this hash table.
The deduplication method signature should be:
`ArrayList<E> hashLinearDeduplication()`
where `E` can be substituted with the specific object you have created to store
a single row.
Besides the list of non-duplicates, also collect the following statistics about
hashing: 1) average number of probes during insertions, 2) max number of probes
during insertions and 3) load factor after insertions. Note that you will need
to update `ProbeHashMap` class to compute these values. Print out these
statistics once after you have inserted all elements into the hashtable in the
following format:
```
Average numer of probes: XXX
Max number of probes: XXX
Load factor: XXX
```

2. Now implement a `DoubleHashMap` class which extends `AbstractHashMap` and
implements a hash table that supports double hashing on collision. Starting
from ProbeHashMap and modifying it to add a secondary hash function is
recommended. Then create a DoubleHashMap and repeat the above.
The deduplication method signature should be:
ArrayList<E> hashDoubleDeduplication()
where E can be substituted with the specific object you have created to store
a single row.
Did the hashing statistics change? Discuss in your README (see details in
Section 2).

#### 1.4.3 Sorting for Deduplication 

The last method for deduplication that we’ll look
at sorts the data to check for duplicates. Note that the comparison method you
use will play an important role in determining if you correctly find the duplicates.
First, we’ll need some functions that allow us to sort.

1. Write a method to perform a quicksort on your data. As usual, object
comparisions should be performed via calls to `.comparedTo`.
2. Java has a library with a buit-in sort method! Write a method that uses
`Collections.sort()` to sort your data.
3. Create two deduplication methods that use these different sorting functions.
The method signatures should be:
`ArrayList<E> quickSortDeduplication()` and
`ArrayList<E> builtinSortDeduplication()`
where `E` can be substituted with the specific object you have created to store
a single row.

## 2. Complexity Analysis

You will now explore the complexity of the deduplication methods you’ve 
developed. The answers to these questions should be given in your README file. 

**Requirements:**

1. How much time (in milliseconds) does each duplication counting method take
when run on the SQF data set?

2. Create a chart (with Excel or any other plotting software) showing the 
number of rows processed on the x-axis and the corresponding time used in
milliseconds on the y-axis for each of these methods (where each method is
a series on the graph). Note that this requires that you collect time data in
your program using the following code snippet that you saw when we first
talked about complexity and analysis of algorithms.

```
long startTime = System.currentTimeMillis();
// run code
long endTime = System.currentTimeMillis();
long elapsed = endTime - startTime;
```

Save the graph and name as `complexity.png`. 
Include this in your submission on gradescope.

3. Explain which method is most efficient in your README, also note which
sorting method (the built-in method or your quicksort method) was more
efficient and hypothesize why this might be.

4. Explain which method is most efficient in your README, also note which
sorting method (the built-in method or your quicksort method) was more
efficient and hypothesize why this might be.

## 3. Command Line INput

You will receive a stop-and-frisk records file on the command line as a single
argument like this:
`2011.csv`
You should process that file as described above and print the following information
out in response.

```
Records given:2000
Attributes checked:X,Y,Z
Duplicates found:100
```

where in this example you were given a file with 2000 lines of police stop-and-
frisk records, determined equality based on attributes X, Y, and Z, and found 100
duplicates in the data (i.e., deduplication returned a list of length 1900). Obviously
you should only call one of your five methods. Use your most efficient one.
Note that we may choose to test your work using data from a different year,
so be sure to actually read the data in from the command line and not hard-code
the file name.

**Requirements:**

1. Take in a CSV file from the command line input.
2. Deduplicate the data and print out the results formatted as above.

## Submitting
Submit the following files to the assignment called `HW07` on Gradescope

1. README: The usual plain text file README
  - Your name:
  - How to compile: 
  - How to run it: 

  - Known Bugs and Limitations: List any known bugs, deficiencies, or 
limitations with respect to the project specifications. Documented bugs
will receive less deduction versus uncaught ones.

  - Discussion: Everything listed above
  - How much time you spent on the assignment
  - What did you learn?
  - (Optional): Any feedback or comments

2. Source files: all .java files

Make sure to name these files exactly what we specify here. Otherwise,
our autograders might not work and we might have to take points off.
<br>
**DO NOT** submit a *.docx* or *.pdf* file.
<br>
**DO NOT** submit any `.class` files or any of the `csv` files.
<br>
**DO NOT** submit any of the provided interfaces. We will use the ones
we provide for testing.

#### Copying the files 
You will likely want to submit the files from your own laptop/computer.
To copy the files from `goldengate` to your own machine, 
open up a terminal on your own computer. If you have a mac or linux, you can use the 
Terminal application, if you have a windows you can use the Powershell app.

Run the following command:

```bassh
$ scp <username>@goldengate.cs.brynmawr.edu/home/<username>/cs151/homeworks/hw00/*
```

Make sure to replace `<username>` with your username. `scp` is a command to secure-copy files
from one server to another. In this case, from `goldengate` to your own machine.

